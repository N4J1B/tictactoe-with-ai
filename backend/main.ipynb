{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266b8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb9ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Menginisialisasi objek Board untuk permainan Tic-Tac-Toe.\n",
    "        Board direpresentasikan sebagai array NumPy 1D berukuran 9.\n",
    "        \"\"\"\n",
    "        self.board = np.zeros(9, dtype=int)  # Board 1D: 0=kosong, 1=Pemain 1, -1=Pemain -1\n",
    "        self.player = 1  # Pemain saat ini: 1 atau -1\n",
    "        self.winner = 0  # Pemenang: 0=belum ada, 1=Pemain 1, -1=Pemain -1, 2=Seri\n",
    "        self.history = []  # Riwayat langkah-langkah board\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Mengatur ulang board ke keadaan awal untuk permainan baru.\n",
    "        \"\"\"\n",
    "        self.board = np.zeros(9, dtype=int)\n",
    "        self.player = 1\n",
    "        self.winner = 0\n",
    "        self.history = []\n",
    "\n",
    "    def available_moves(self):\n",
    "        \"\"\"\n",
    "        Mengembalikan daftar indeks langkah yang tersedia (sel yang kosong).\n",
    "        \"\"\"\n",
    "        return [i for i, x in enumerate(self.board) if x == 0]\n",
    "\n",
    "    def make_move(self, index):\n",
    "        \"\"\"\n",
    "        Melakukan langkah pada indeks yang diberikan.\n",
    "        Mengembalikan True jika langkah valid, False jika tidak.\n",
    "        \"\"\"\n",
    "        if 0 <= index < 9 and self.board[index] == 0:\n",
    "            self.board[index] = self.player\n",
    "            self.history.append(tuple(self.board)) # Menyimpan keadaan board ke riwayat\n",
    "            self.player *= -1\n",
    "            self.check_winner()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"\n",
    "        Memeriksa apakah ada pemenang atau permainan seri.\n",
    "        Memperbarui atribut self.winner.\n",
    "        \"\"\"\n",
    "        winning_combinations = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8], # Rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8], # Columns\n",
    "            [0, 4, 8], [2, 4, 6]             # Diagonals\n",
    "        ]\n",
    "\n",
    "        for combo in winning_combinations:\n",
    "            s = self.board[combo[0]] + self.board[combo[1]] + self.board[combo[2]]\n",
    "            if abs(s) == 3:\n",
    "                self.winner = s // 3\n",
    "                return\n",
    "\n",
    "        if len(self.available_moves()) == 0 and self.winner == 0:\n",
    "            self.winner = 2\n",
    "\n",
    "    def get_board_state(self):\n",
    "        \"\"\"\n",
    "        Mengembalikan keadaan board saat ini sebagai tuple (immutable).\n",
    "        \"\"\"\n",
    "        return tuple(self.board)\n",
    "\n",
    "    def get_player_turn(self):\n",
    "        \"\"\"\n",
    "        Mengembalikan giliran pemain saat ini (1 atau -1).\n",
    "        \"\"\"\n",
    "        return self.player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent: # Nama kelas diubah menjadi QAgent untuk membedakan\n",
    "    def __init__(self, name, player_id, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_rate=0.9999, alpha=0.9, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Menginisialisasi objek QAgent untuk bermain Tic-Tac-Toe menggunakan Tabular Q-Learning.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Nama agen.\n",
    "            player_id (int): ID pemain yang diwakili agen (1 atau -1).\n",
    "            epsilon_start (float): Nilai epsilon awal untuk eksplorasi.\n",
    "            epsilon_end (float): Nilai epsilon minimum.\n",
    "            epsilon_decay_rate (float): Tingkat peluruhan epsilon per episode.\n",
    "            alpha (float): Tingkat pembelajaran (learning rate).\n",
    "            gamma (float): Faktor diskon untuk nilai masa depan.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.player_id = player_id\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {}  # Kamus untuk menyimpan Q-value: {state_tuple: {action_index: Q_value}}\n",
    "        self.history_state_actions = [] # Riwayat (state, action) yang diambil agen dalam satu episode\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Mengambil Q-value untuk state-action pair.\"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {i: 0.0 for i in range(9)} # Inisialisasi Q-value untuk semua aksi menjadi 0.0\n",
    "        return self.q_table[state].get(action, 0.0) # Mengembalikan 0.0 jika aksi belum ada\n",
    "\n",
    "    def choose_action(self, board_obj):\n",
    "        \"\"\"\n",
    "        Memilih langkah berdasarkan strategi epsilon-greedy menggunakan Q-table.\n",
    "        \n",
    "        Args:\n",
    "            board_obj (Board): Objek Board permainan saat ini.\n",
    "            \n",
    "        Returns:\n",
    "            int: Indeks langkah yang dipilih.\n",
    "        \"\"\"\n",
    "        current_state = board_obj.get_board_state()\n",
    "        available_moves = board_obj.available_moves()\n",
    "        \n",
    "        if not available_moves:\n",
    "            return None\n",
    "\n",
    "        # Eksplorasi: Pilih langkah acak dengan probabilitas epsilon\n",
    "        if np.random.uniform(0, 1) <= self.epsilon:\n",
    "            action = np.random.choice(available_moves)\n",
    "        else:\n",
    "            # Eksploitasi: Pilih langkah dengan nilai Q-value tertinggi\n",
    "            q_values_for_moves = {}\n",
    "            for move_index in available_moves:\n",
    "                q_values_for_moves[move_index] = self.get_q_value(current_state, move_index)\n",
    "            \n",
    "            # Cari Q-value maksimum\n",
    "            max_q_value = -float('inf')\n",
    "            for q_val in q_values_for_moves.values():\n",
    "                if q_val > max_q_value:\n",
    "                    max_q_value = q_val\n",
    "            \n",
    "            # Kumpulkan semua aksi yang memiliki Q-value maksimum\n",
    "            best_actions = [move for move, q_val in q_values_for_moves.items() if q_val == max_q_value]\n",
    "            \n",
    "            # Pilih satu aksi secara acak dari yang terbaik (untuk tie-breaking)\n",
    "            action = np.random.choice(best_actions)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def add_state_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Menambahkan pasangan (state, action) yang diambil agen ke riwayat.\n",
    "        \"\"\"\n",
    "        self.history_state_actions.append((state, action))\n",
    "\n",
    "    def learn(self, board_obj):\n",
    "        \"\"\"\n",
    "        Memperbarui nilai Q-value berdasarkan hasil permainan (reward) menggunakan Q-learning.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        if board_obj.winner == self.player_id:\n",
    "            reward = 1      # Agen menang\n",
    "        elif board_obj.winner == -self.player_id:\n",
    "            reward = -1     # Agen kalah\n",
    "        elif board_obj.winner == 2:\n",
    "            reward = 0.5    # Permainan seri\n",
    "\n",
    "        # Iterasi mundur melalui riwayat (state, action)\n",
    "        # Dengan Q-Learning, kita memperbarui Q(S,A) berdasarkan Q(S',A')\n",
    "        # S_t+1 adalah state setelah aksi A_t, A_t+1 adalah aksi optimal di S_t+1\n",
    "        \n",
    "        # Q-learning off-policy: Q(S,A) <- Q(S,A) + alpha * [R + gamma * max_a' Q(S',a') - Q(S,A)]\n",
    "        \n",
    "        # Jika ini adalah state terminal, reward langsung diberikan, tidak ada Q(S',A')\n",
    "        \n",
    "        # Dapatkan state_action terakhir dari history\n",
    "        # Perhatikan bahwa history_state_actions sudah dalam urutan kronologis.\n",
    "        # Kita akan iterasi dari yang terakhir (state sebelum terminal) ke yang pertama.\n",
    "\n",
    "        # Nilai Q max untuk state terminal adalah 0 (karena tidak ada aksi lagi)\n",
    "        next_q_max = 0.0\n",
    "\n",
    "        # Iterasi mundur melalui riwayat (state, action)\n",
    "        for state, action in reversed(self.history_state_actions):\n",
    "            # Pastikan state ada di q_table\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = {i: 0.0 for i in range(9)}\n",
    "\n",
    "            # Hitung TD Target\n",
    "            if next_q_max is None: # Ini hanya akan terjadi untuk state terminal\n",
    "                td_target = reward\n",
    "            else:\n",
    "                td_target = reward + self.gamma * next_q_max\n",
    "            \n",
    "            # Hitung TD Error\n",
    "            td_error = td_target - self.get_q_value(state, action)\n",
    "            \n",
    "            # Perbarui Q-value\n",
    "            self.q_table[state][action] = self.get_q_value(state, action) + self.alpha * td_error\n",
    "            \n",
    "            # Untuk iterasi berikutnya, next_q_max adalah Q-value dari state saat ini yang baru diperbarui\n",
    "            # Namun, ini adalah Q-value dari state-action PAIR yang baru saja diambil.\n",
    "            # Q-learning menggunakan max_a' Q(S',a')\n",
    "            # Jadi, next_q_max untuk langkah sebelumnya adalah Q-value tertinggi dari state 'state' saat ini.\n",
    "            \n",
    "            # Jika state saat ini bukan state awal permainan dan bukan state terminal\n",
    "            # Kita perlu mencari max Q(S,a) dari state 'state' untuk semua aksi yang mungkin.\n",
    "            # Ini adalah estimasi terbaik dari nilai state 'state' itu sendiri.\n",
    "            next_q_max = max(self.q_table[state].values()) # Q-value optimal dari state 'state'\n",
    "\n",
    "            # Set reward untuk langkah selanjutnya menjadi 0 (reward hanya di akhir episode)\n",
    "            reward = 0 \n",
    "            \n",
    "        self.history_state_actions = [] # Mengosongkan riwayat setelah pembelajaran\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Memperbarui nilai epsilon (untuk eksplorasi) berdasarkan episode.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_start * (self.epsilon_decay_rate ** episode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'tictactoe_agent_x.pkl'\n",
    "filename2 = 'tictactoe_agent_o.pkl'\n",
    "\n",
    "try:\n",
    "    with open(filename, 'rb') as f:\n",
    "        agent_x = pickle.load(f) # Load agent\n",
    "    print(f\"Agen AI berhasil diload ke '{filename}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat Load agen: {e}\")\n",
    "    agent_x = Agent(name=\"Q-Agent X\", player_id=1, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_rate=0.99995, alpha=0.1, gamma=0.9)\n",
    "try:\n",
    "    with open(filename2, 'rb') as f:\n",
    "        agent_o = pickle.load(f) # Load agent_x\n",
    "    print(f\"Agen AI berhasil diload ke '{filename2}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat Load agen: {e}\")\n",
    "    agent_o = Agent(name=\"Q-Agent O\", player_id=-1, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_rate=0.99995, alpha=0.1, gamma=0.9)\n",
    "\n",
    "# Inisialisasi agen\n",
    "# Epsilon decay rate disesuaikan sedikit untuk eksplorasi lebih lama\n",
    "\n",
    "# Inisialisasi board permainan\n",
    "game_board = Board()\n",
    "\n",
    "num_episodes = 50000 # Jumlah episode pelatihan yang ditingkatkan\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    game_board.reset() # Reset board untuk setiap episode baru\n",
    "    current_agent = None\n",
    "\n",
    "    # Loop permainan sampai ada pemenang atau seri\n",
    "    while game_board.winner == 0 and len(game_board.available_moves()) > 0:\n",
    "        current_player_id = game_board.get_player_turn()\n",
    "        \n",
    "        if current_player_id == 1:\n",
    "            current_agent = agent_x\n",
    "        else:\n",
    "            current_agent = agent_o\n",
    "        \n",
    "        current_state = game_board.get_board_state()\n",
    "        action = current_agent.choose_action(game_board)\n",
    "\n",
    "        if action is not None:\n",
    "            current_agent.add_state_action(current_state, action) # Tambahkan (state, action) ke riwayat agen\n",
    "            game_board.make_move(action) # Lakukan langkah di board\n",
    "        else:\n",
    "            # Ini bisa terjadi jika tidak ada langkah yang tersedia (permainan sudah selesai)\n",
    "            # atau jika ada bug yang menyebabkan agen memilih None.\n",
    "            break \n",
    "    \n",
    "    # Setelah permainan selesai, agen belajar dari hasilnya\n",
    "    agent_x.learn(game_board)\n",
    "    agent_o.learn(game_board)\n",
    "\n",
    "    # Update epsilon untuk setiap agen\n",
    "    agent_x.update_epsilon(episode)\n",
    "    agent_o.update_epsilon(episode)\n",
    "\n",
    "    if (episode + 1) % 5000 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} selesai. Epsilon Agen X: {agent_x.epsilon:.4f}, Epsilon Agen O: {agent_o.epsilon:.4f}\")\n",
    "\n",
    "print(\"\\n--- Pelatihan Selesai ---\")\n",
    "print(f\"Jumlah state yang dipelajari Agen X: {len(agent_x.q_table)}\")\n",
    "print(f\"Jumlah state yang dipelajari Agen O: {len(agent_o.q_table)}\")\n",
    "\n",
    "try:\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(agent_x, f) # Simpan agent1\n",
    "    print(f\"Agen AI berhasil disimpan ke '{filename}'\")\n",
    "    with open(filename2, 'wb') as f:\n",
    "        pickle.dump(agent_o, f) # Simpan agent1\n",
    "    print(f\"Agen AI berhasil disimpan ke '{filename2}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat menyimpan agen: {e}\")    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
